{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d14685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VaderSentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk import tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837029fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8586ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba55307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4bde1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    vowels = 'aeiouAEIOU'\n",
    "    exceptions = ['es', 'ed']\n",
    "    count = 0\n",
    "\n",
    "    # Handle exceptions\n",
    "    if word.endswith(tuple(exceptions)):\n",
    "        return count\n",
    "\n",
    "    if len(word) > 0 and word[0] in vowels:\n",
    "        count += 1\n",
    "\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "\n",
    "    if count == 0 and len(word) > 0:\n",
    "        count += 1\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e64d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file containing the URLs\n",
    "file_path = 'C:/Users/shiva/Documents/Assignment/Input.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Extract the URLs from the Excel file\n",
    "urls = df['URL'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c168cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file containing stopwords\n",
    "stopwords_file_path = 'C:/Users/shiva/Documents/Assignment/Stopwords.csv'\n",
    "stopwords_df = pd.read_csv(stopwords_file_path, encoding='latin-1')\n",
    "\n",
    "# Extract the stopwords from the CSV file\n",
    "stopwords_list = stopwords_df['Stopwords'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27df08cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file containing negative words\n",
    "negative_words_file_path = 'C:/Users/shiva/Documents/Assignment/MasterDictionary/negative-words.txt'\n",
    "with open(negative_words_file_path, 'r') as file:\n",
    "    negative_words = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e2c8c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file containing positive words\n",
    "positive_words_file_path = 'C:/Users/shiva/Documents/Assignment/MasterDictionary/positive-words.txt'\n",
    "with open(positive_words_file_path, 'r') as file:\n",
    "    positive_words = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc775ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzers\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "vader_sia = VaderSentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results list\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23c4dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the personal pronouns regex pattern\n",
    "pronouns_pattern = r\"\\b(I|we|my|ours|us)\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34910109",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to: C:/Users/shiva/Documents/Assignment/Output Data Structure.xlsx\n"
     ]
    }
   ],
   "source": [
    "for url in urls:\n",
    "    # Send a request to the URL and retrieve the web page contents\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    # Use BeautifulSoup to extract the text from HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "     # Initialize variables\n",
    "    cleaned_words = []\n",
    "    personal_pronoun_count = 0\n",
    "    # Initialize variables\n",
    "    syllable_count = 0\n",
    "    # Initialize scores\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    polarity_score = 0\n",
    "    subjectivity_score = 0\n",
    "    word_count = 0\n",
    "    complex_word_count = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # Tokenize the sentence into words\n",
    "        tokens = tokenize.word_tokenize(sentence)\n",
    "        # Remove stopwords, negative words, and positive words from the tokens\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token.lower() not in negative_words and token.lower() not in positive_words]\n",
    "        # Add cleaned words to the list\n",
    "        cleaned_words.extend(filtered_tokens)\n",
    "        # Count personal pronouns\n",
    "        personal_pronoun_count += len(re.findall(pronouns_pattern, ' '.join(filtered_tokens), re.IGNORECASE))\n",
    "        # Update word and complex word counts\n",
    "        word_count += len(filtered_tokens)\n",
    "        complex_word_count += len([token for token in filtered_tokens if len(token) > 2])\n",
    "        \n",
    "        # Count syllables in each word\n",
    "        for word in filtered_tokens:\n",
    "            syllable_count += count_syllables(word)\n",
    "        \n",
    "        # Calculate scores\n",
    "        if filtered_tokens:\n",
    "            sentiment = sia.polarity_scores(' '.join(filtered_tokens))\n",
    "            vader_sentiment = vader_sia.polarity_scores(' '.join(mark_negation(filtered_tokens)))\n",
    "            positive_score += vader_sentiment['pos']\n",
    "            negative_score += vader_sentiment['neg']\n",
    "            polarity_score += sentiment['compound']\n",
    "            subjectivity_score += sentiment['neu'] + sentiment['pos'] + sentiment['neg']\n",
    "            \n",
    "\n",
    "    # Calculate readability metrics\n",
    "    average_sentence_length = word_count / len(sentence)\n",
    "    percentage_complex_words = (complex_word_count / word_count) * 100\n",
    "    Fog_Index = 0.4 *(average_sentence_length + percentage_complex_words)\n",
    "\n",
    "    # Average word count per sentence\n",
    "    average_words_per_sentence = word_count / len(sentence)\n",
    "\n",
    "    # Calculate complex words\n",
    "    complex_words = [word for word in cleaned_words if count_syllables(word) > 2]\n",
    "    complex_word_count = len(complex_words)\n",
    "\n",
    "    # Count the total cleaned words\n",
    "    total_cleaned_words = len(cleaned_words)\n",
    "\n",
    "    # Calculate average word length\n",
    "    total_characters = sum(len(word) for word in cleaned_words)\n",
    "    average_word_length = total_characters / len(cleaned_words)\n",
    "\n",
    "    # Create a dictionary of results for the current URL\n",
    "    result = {\n",
    "        'URL': url,\n",
    "        'Positive Score': positive_score,\n",
    "        'Negative Score': negative_score,\n",
    "        'Polarity Score': polarity_score,\n",
    "        'Subjectivity Score': subjectivity_score,\n",
    "        'Average Sentence Length': average_sentence_length,\n",
    "        'Percentage Complex Words': percentage_complex_words,\n",
    "        'Fog Index': Fog_Index,\n",
    "        'Average Words Per Sentence': average_words_per_sentence,\n",
    "        'Total Cleaned Words': total_cleaned_words,\n",
    "        'Complex Word Count': complex_word_count,\n",
    "        'Syllable Count': syllable_count,\n",
    "        'Personal Pronoun Count': personal_pronoun_count,\n",
    "        'Average Word Length:': average_word_length\n",
    "    }\n",
    "    \n",
    "    # Append the result to the results list\n",
    "    results.append(result)\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to an XLSX file\n",
    "output_file_path = 'C:/Users/shiva/Documents/Assignment/Output Data Structure.xlsx'\n",
    "df_results.to_excel(output_file_path)\n",
    "\n",
    "print(\"Output saved to:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a849d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
